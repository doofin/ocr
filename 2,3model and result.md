整个模型由基本单元LSTM，损失函数CTC，还有为了防止过拟合的Dropout层组成。

模型的基本神经元LSTM是迭代神经网络的一种。关于一般的迭代神经网络已经在之前介绍。LSTM的优点是能够“记忆”比较长时间段内的函数（因果）关系，比如对于语音识别，在预测语音“Hello”时，先预测到了H，则LSTM能够结合"e"的发音，以及刚刚已经预测到的“H”，来更好的识别出字符“e”，而且在预测更后面的字符时仍然能够结合预测到“H”这一背景信息。LSTM的长时间记忆特性在unresonable effectness of rnn[]中得到了很好的展示，文章中介绍了输入某个作家的文字作品到LSTM神经网络进行训练，然后该网络竟然可以生成“文风”和内容都相近的文章。下面来介绍一下LSTM的数学描述，以及其为什么能够实现长时间依赖的“记忆”：

介绍完基本单元后，这里开始描述由基本单元组成神经网络的前面部分。模型采用了5层lstm，每层的隐藏神经数依次递减，这样的设置是因为底层（更接近输入层）的像素组合非常多，需要更多神经元来提供足够的容量去拟合，而接近输出层的网络参数较少（英文大小写字母加上空格一共53类），所以用较少层数即可。
最后一层是CTC损失函数，这是alex.graves[]提出的，其输出的数量（神经网络执行一次输出的已经识别的字符）可以小于等于输入（一个输入是一列像素，宽度为1像素，高度为字符图片高度）的个数，这是无分割识别和训练最关键的地方。这里的输入个数远远大于输出的个数，即输入很多列宽度为1的像素，因此一句话的输入可能有上百个，而输出的字符数量只有几十。在CTC之前主要应用的是HMM损失函数。

训练算法：反向传播
数据集


